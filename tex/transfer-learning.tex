\chap Transfer Learning

A common assumption in machine learning is that the training and data faced in
deployment must be in same feature space and have the same probability
distribution.
For example, this thesis is interested in classification of LAMOST archive
but has training set from Ond≈ôejov spectrograph.
In such cases as much knowledge as possible need to be transfer to
maximally improve performance of final classifier. \cite[transfer2010]

\midinsert \clabel[transfer-learning]{Transfer Learning}
\picw=7.5cm \cinspic img/transfer-learning.png
\caption/f Diagram of transfer learning process.
Inspired by \cite[transfer2010].
\endinsert

Formal definition of transfer learning is provided in \cite[transfer2010].
{\bf Domain} $\cal D$ consists of a feature space $\cal X$
and a probability distribution $P(X)$
where $X = \{x_1, ..., x_n\} \in \cal X$ is a learnin sample.
Given a specific ${\cal D} = \{{\cal X}, P(X)\}$
and a learning {\bf task} ${\cal T} = \{{\cal Y}, f\}$ where
$\cal Y$ is a label space and $f$ is a predictive function.
This function is learning from training set which consist of pairs
$\{x_i \in X, y_i \in {\cal Y}\}$.
From probabilistic perspective $f$ is $P(y \mid x)$.

Therefore, having a source domain ${\cal D}_S$ and a task ${\cal T}_S$,
a source domain ${\cal D}_T$ and a task ${\cal T}_T$,
{\bf transfer learning} amis to improve the performace of function $f_T$ in
${\cal D}_T$ using knowledge in ${\cal D}_S$ and ${\cal T}_S$,
when ${\cal D}_S \ne {\cal D}_T$ or ${\cal T}_S \ne {\cal T}_T$.

In \cite[ruder2017] transfer learning is divided into four scenarios
according to how source and target conditions vary:

\begitems \style n
* ${\cal X}_S \ne {\cal X}_T$, feature spaces of domains are different,
e.g. in one domain picture is represented in pixels
and in the other domain is described with text.
* $P(X_S) \ne P(X_T)$, probability distribution are different, e.g. two spectral
archive observe different kinds of stars.
* ${\cal Y}_S \ne {\cal Y}_T$, labels of tasks are not the same,
e.g. spectra should be assign different labels in target task.
This usually occurs with the fourth scenario.
* $P(Y_S \mid X_S) \ne P(Y_T \mid X_T)$, probability distribution of tasks are
different.
\enditems

\sec Domain Adaptation

TODO

\sec Gaussian Blur

TODO
