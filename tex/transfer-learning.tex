\chap Transfer Learning

A common assumption in machine learning is that the training and data faced in
deployment must be in same feature space and have the same probability
distribution.
For example, this thesis is interested in classification of \glref{LAMOST}
archive but has training set from Ond≈ôejov spectrograph.
In such cases as much knowledge as possible need to be transfer to
maximally improve performance of final classifier. \cite[transfer2010]

\midinsert \clabel[transfer-learning]{Transfer Learning}
\picw=7.5cm \cinspic img/transfer-learning.png
\caption/f Diagram of transfer learning process.
Inspired by \cite[transfer2010].
\endinsert

Formal definition of transfer learning is provided in \cite[transfer2010].
{\bf Domain} $\cal D$ consists of a feature space $\cal X$
and a probability distribution $P(X)$
where $X = \{x_1, ..., x_n\} \in \cal X$ is a learning sample.
Given a specific ${\cal D} = \{{\cal X}, P(X)\}$
and a learning {\bf task} ${\cal T} = \{{\cal Y}, f\}$ where
$\cal Y$ is a label space and $f$ is a predictive function.
This function is learning from training set which consist of pairs
$\{x_i \in X, y_i \in {\cal Y}\}$.
From probabilistic perspective $f$ is $P(y \mid x)$.

Therefore, having a source domain ${\cal D}_S$ and a task ${\cal T}_S$,
a target domain ${\cal D}_T$ and a task ${\cal T}_T$,
{\bf transfer learning} amis to improve the performace of function $f_T$
from ${\cal T}_T$ using knowledge from ${\cal D}_S$ and ${\cal T}_S$,
when ${\cal D}_S \ne {\cal D}_T$ or ${\cal T}_S \ne {\cal T}_T$.

In \cite[ruder2017] transfer learning is divided into four scenarios
according to how source and target conditions vary:

\begitems \style n
* ${\cal X}_S \ne {\cal X}_T$, feature spaces of domains are different,
e.g. in one domain picture is represented in pixels
and in the other domain is described by text.
* $P(X_S) \ne P(X_T)$, probability distribution of source and target domains
are different, e.g. two spectral archive observe different kinds of stars.
* ${\cal Y}_S \ne {\cal Y}_T$, labels of tasks are not the same,
e.g. spectra should be assign different labels in target task.
This usually occurs with the fourth scenario.
* $P(Y_S \mid X_S) \ne P(Y_T \mid X_T)$, probability distribution of tasks are
different.
\enditems

\sec Domain Adaptation

In transfer learning the case when probability distribution of source
and target domains are different, $P(X_S) \ne P(X_T)$, is generally known as
{\bf domain adaptation}. \cite[ruder2017]

Thus domain adaptation considers the setting in which the training and
testing data are sampled from different distibutions.
The learning problem consists of finding a function realizing a good
tranfer of knowledge from ${\cal D}_S$ to ${\cal D}_T$.
That means the model trained on data drawn from distribution $P(X_S)$
generalizes well on data drawn from $P(X_T)$. \cite[domain-adaptation]
