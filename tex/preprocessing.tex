\chap Preprocessing

This chapter describes all the action taken on the data from Ondřejov
dataset before processing them through a neural network.
The last section is about dimensionality reduction of the data
and its visualizations.

\sec Knowledge Transfer

Ondřejov and \glref{LAMOST} spectrographs have different parameters
(as described in sections~\ref[ondrejov-archive] and
\ref[lamost-archive]).
Thus, spectra from the two archive different.
Concretely, spectra from Ondřejov archive has much more details than
spectra from \glref{LAMOST}spectrograph
and spectra in Ondřejov archive are stored in air wavelengths
but \glref{LAMOST} store spectra in vacuum wavelengths.

This thesis is concerned with classifying \glref{LAMOST} data using
data from Ondřejov.
It aims to extract as much knowledge as possible from Ondřejov data
and apply it to classification of \glref{LAMOST} archive.
Transfer learning and domain adaptation
(described in chapter~\ref[transfer-learning])
are study area of machine learning which aim to offer methods
for dealing with these problems.

The following sections introduce methods
that were applied to Ondřejov data.
These methods make the two domains more similar and so
they deal with differences between the archives mentioned above.

\secc Wavelength Conversion

Ondřejov and \glref{LAMOST} archives store wavelengths in air and vacuum
wavelength respectively.
When spectra of same object from the two archives are plotted on each
other they are a bit shifted (see top plot in figure~\ref[air2vacuum]).
Therefore, Ondřejov spectra are converted to vacuum wavelengths
according to formulas provided in \cite[air2vacuum]:

$$ \lambda_{v} = n \lambda_{a} \eqmark $$

$$
n = 1 + 8.34254 \cdot 10^{-5} +
    {2.406147 \cdot 10^{-2} \over (130 - s^2)} +
    {1.5998 \cdot 10^{-4} \over (38.9 - s^2)}
\eqmark
$$

$$ s = {10^4 \over \lambda_{a}} \eqmark $$

where $\lambda_{v}$ is vacuum wavelength and $\lambda_{a}$ is
corresponding air wavelength.

The resulting Ondřejov spectrum after air to vacuum conversion
is plotted in bottom plot in figure~\ref[air2vacuum].

\midinsert \clabel[air2vacuum]{Air to vacuum conversion}
\picw=15cm \cinspic img/air2vacuum.png
\caption/f Spectrum of object HIP47636 from Ondřejov archive before
and after conversion of air to vacuum wavelengths in comparison with
spectrum of the same object from \glref{LAMOST} archive.
\endinsert

\secc Gaussian Blur

According to \cite[szeliski2010] Gaussian filter smooths away high-frequency
detail of images.
Spectrum can be seen as one dimensional image.
Correctly parameterized Gaussian blur applied to Ondřejov spectrum
would reduce the amount of detail
and thus make it more similar to spectra from \glref{LAMOST} archive.

The equation of a one dimensional Gaussian function:

$$ G(x) = {1 \over \sqrt{2 \pi \sigma^2}} e^{-{x^2 \over 2 \sigma^2}} \eqmark $$

where $\sigma$ is standard deviation in pixels.
Kernel generated by this function is convolved with a spectrum
resulting in Gaussian blur.

This work uses convolution implementation from Python package astropy
\cite[astropy] in version 1.3.1.
Standard deviation of value 7 was chosen after visualizing results of
convolutions.
Following code implements functionality described above:

\begtt
from astropy.convolution import Gaussian1DKernel, convolve

gauss_kernel = Gaussian1DKernel(stddev=7)
smoothed_fluxes = convolve(fluxes, gauss_kernel)
\endtt

where {\tt fluxes} is array of spectrum's fluxes.
Convolved spectrum's fluxes are stored to {\tt smoothed\_fluxes}.

\midinsert \clabel[gaussian-blur-plot]{Gaussian blur}
\picw=15cm \cinspic img/gaussian-blur.png
\caption/f Comparison of original BT CMi and V395 Aur spectra from LAMOST
and Ondřejov and Ondřejov's spectra convolved by Gaussian kernel
with standard deviation of value 7.
\endinsert

\sec Regridding

To train, validate and test deep neural network
two dimensional dataset matrices needs to be created.
In a matrix each row is a spectrum sample
and each column represents flux in certain wavelength.
That means that all spectra need to be regrided to same wavelengths.

The wavelengths range is given by Ondřejov dataset from 6519 to 6732
(minimum is rounded up and maximum is rounded down to nearest integer value).
Ondřejov spectra in this range has 829, 830, 831, or 922, 923 measured fluxes
according to this thesis's experiments.
This variance is caused by change of spectrograph in Ondřejov observatory
(see section~\ref[ondrejov-archive])
and the small deviation by the cut of spectrum into the range.
LAMOST spectra has 140 measurements in this range
inferred from 22 cross-matched spectra.

Because Ondřejov spectra are moved to \glref{LAMOST} resolution,
after air to vacuum conversion and convolution,
spectra are regrided uniformly to 140 points in range 6519 to 6732.
To do regridding linear interpolation is used.
Here is NumPy 1.12.1 \cite[numpy] code carrying out this operation:

\begtt
import numpy as np

new_wavelengths = np.linspace(6519, 6732, num=140)
new_fluxes = np.interp(new_wavelengths, old_wavelengths, old_fluxes)
\endtt

where {\tt old\_wavelengths} variable holds old wavelengths
and {\tt old\_fluxes} variable holds corresponding fluxes of a spectrum.

\sec Dimensionality Reduction

After the preprocessing methods mentioned in above sections each spectrum
is a point in 140-dimensional space.
To better understant the data \glref{PCA} and \glref{t-SNE} dimensionality
reduction algorithms are applied to reduce each point to 2-dimensional space
which can be visualized as scatter plot.

\secc PCA

Principal Component Analysis is statistical technique to tranform a number
of possibly correlated variables into a smaller number of variables
called principal components. \cite[pca]

Left plot in figure~\ref[pca-plot] displays first and second principal
component of original dataset.
After further analysis the plot express more the intensities in spectra's
fluxes than the shape.
The points far from the dense area in middle left have high intense fluxes.
But in this work the difference between intesities in fluxes is not interesting.

\midinsert \clabel[pca-plot]{PCA}
\picw=15cm \cinspic img/pca.png
\caption/f On the left size is scatter plot with first two principal
components of the original dataset. The plot more that difference in shape
show difference in fluxes values.
Right plot displays result of PCA applied to scaled variant of the dataset.
After such preprocessing mehtod absorption spectra are oriented in left part
of the plot, emission spectra are on the other side and double-peak spectra
are spread across the whole plot.
\endinsert

The other plot in figure~\ref[pca-plot] shows dataset with supressed intensities
so that the only feature should be the shape of each spectra
(spectra are classified according to their shapes see chapter~\ref[ondrejov].
Scaling of spectra intesities is done with Scikit-learn 0.18.1 \cite[sklearn]: 

\begtt
import sklearn.preprocessing

X_scaled = sklearn.preprocessing.scale(X_orig, axis=1)
\endtt

where {\tt X\_orig} is input matrix of size $n \times 140$
($n$ is number of spectra samples) containing original dataset.
Each row of the matrix are the 140 fluxes of a spectrum.
This procedure will scale each spectrum to have zero mean and unit varince
and so the intensities are supressed.

The scatter plot then expresses the structure of the dataset clearly.
Absroption spectra are on the left side
while emission spectra are mostly on the other side.
Double-peak spectra are spread across the whole plot.
This may implies that the emission and absorption spectra can be separable
by linear classifier.
But the double-peak spectra are mixed up with the other classes so
hopefully a deep neural network can find such representation that all
classes are separable.

\secc t-SNE

Other popular method of dimensionality reduction is t-Distributed Stochastic
Neighbor Embedding. \glref{t-SNE} is TODO.
\cite[tsne]

\midinsert \clabel[tsne-plot]{TODO}
\picw=15cm \cinspic img/tsne.png
\caption/f TODO
\endinsert
