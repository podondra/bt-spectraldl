\chap Preprocessing

This chapter describes all preprocessing methods applied on
Ondřejov dataset before processing them through a neural network.

\sec Knowledge Transfer

Ondřejov and \glref{LAMOST} spectrographs have different parameters
(as described in sections~\ref[ondrejov-archive] and
\ref[lamost-archive]).
Thus, spectra from the two archive are different.
Concretely, spectra from Ondřejov archive has much more details than
spectra from \glref{LAMOST} spectrograph
and spectra in Ondřejov archive are stored in air wavelengths
but \glref{LAMOST} store spectra in vacuum wavelengths.

This thesis is concerned with classifying \glref{LAMOST} data using
data from Ondřejov.
It aims to extract as much knowledge as possible from Ondřejov data
and apply it to classification of \glref{LAMOST} archive.
Transfer learning and domain adaptation
(described in chapter~\ref[transfer-learning])
are study area of machine learning that aim to offer methods
for dealing with these problems.

The following sections introduce methods
that were applied to Ondřejov data.
These methods make the two domains more similar and so
they deal with differences between the archives mentioned above.

\secc Wavelength Conversion

Ondřejov and \glref{LAMOST} archives store wavelengths in air and vacuum
wavelength respectively.
When spectra of same object from the two archives are plotted on each
other they are a bit shifted (see top plot in figure~\ref[air2vacuum]).
Therefore, Ondřejov spectra are converted to vacuum wavelengths
according to formulas provided in \cite[air2vacuum]:

$$ \lambda_{v} = n \lambda_{a} \eqmark $$

$$
{\rm where}\quad n = 1 + 8.34254 \cdot 10^{-5} +
    {2.406147 \cdot 10^{-2} \over (130 - s^2)} +
    {1.5998 \cdot 10^{-4} \over (38.9 - s^2)}
\eqmark
$$

$$ {\rm and}\quad s = {10^4 \over \lambda_{a}} \eqmark $$

where $\lambda_{v}$ is vacuum wavelength and $\lambda_{a}$ is
corresponding air wavelength.

The resulting Ondřejov spectrum after air to vacuum conversion
is plotted in bottom plot in figure~\ref[air2vacuum].

\midinsert \clabel[air2vacuum]{Air to vacuum conversion}
\picw=15cm \cinspic img/air2vacuum.png
\caption/f Spectrum of object HIP47636 from Ondřejov archive before
and after conversion of air to vacuum wavelengths in comparison with
spectrum of the same object from \glref{LAMOST} archive.
\endinsert

\secc Gaussian Blur

According to \cite[szeliski2010] Gaussian filter smooths away high-frequency
detail of images.
Spectrum can be seen as one dimensional image.
Correctly parameterized Gaussian blur applied to Ondřejov spectrum
would reduce the amount of detail
and thus make it more similar to spectra from \glref{LAMOST} archive.

The equation of a one dimensional Gaussian function:

$$ G(x) = {1 \over \sqrt{2 \pi \sigma^2}} e^{-{x^2 \over 2 \sigma^2}} \eqmark $$

where $\sigma$ is standard deviation in pixels.
Kernel generated by this function is convolved with a spectrum
resulting in Gaussian blur.

This work uses convolution implementation from Python package astropy
\cite[astropy] in version 1.3.1.
Standard deviation of value 7 was chosen after visualizing results of
convolutions (see figure~\ref[gaussian-blur-plot]).
Following code implements functionality described above:

\begtt
from astropy.convolution import Gaussian1DKernel, convolve

gauss_kernel = Gaussian1DKernel(stddev=7)
smoothed_fluxes = convolve(fluxes, gauss_kernel)
\endtt

where {\tt fluxes} is array of spectrum's fluxes.
Convolved spectrum's fluxes are stored to {\tt smoothed\_fluxes}.

\midinsert \clabel[gaussian-blur-plot]{Gaussian blur}
\picw=15cm \cinspic img/gaussian-blur.png
\caption/f Comparison of original BT CMi and V395 Aur spectra from LAMOST
and Ondřejov and spectra from Ondřejov convolved by Gaussian kernel
with standard deviation of value 7.
\endinsert

\sec Regridding

To train, validate and test deep neural network
two dimensional dataset matrices needs to be created.
In a matrix each row is a spectrum sample
and each column represents flux in certain wavelength.
That means that all spectra need to be regrided to same grid of wavelengths.

The wavelengths range is given by Ondřejov dataset from 6519 to 6732
(minimum is rounded up and maximum is rounded down to nearest integer value).
Ondřejov spectra in this range have 829, 830, 831, or 922, 923 measured fluxes
according to this thesis's experiments.
This variance is caused by change of spectrograph in Ondřejov observatory
(see section~\ref[ondrejov-archive])
and the small deviation by the cut of spectrum into the range.
LAMOST spectra has 140 measurements in this range
inferred from 22 cross-matched spectra.

Because Ondřejov spectra are moved to \glref{LAMOST} resolution,
after air to vacuum conversion and convolution,
spectra are regrided uniformly to 140 points in range 6519 to 6732.
To do regridding linear interpolation is used.
Here is NumPy 1.12.1 \cite[numpy] code carrying out this operation:

\begtt
import numpy as np

new_wavelengths = np.linspace(6519, 6732, num=140)
new_fluxes = np.interp(new_wavelengths, old_wavelengths, old_fluxes)
\endtt

where {\tt old\_wavelengths} variable holds old wavelengths
and {\tt old\_fluxes} variable holds corresponding fluxes of a spectrum.

\sec Spectra Scaling

Spectra in Ondřejov dataset were classified according to their shape.
But the spectra fluxes has different intensities.
Especially, emission spectra tend to have high fluxes.
It causes that spectra do not map to same space after dimensionality reduction
(see scatter plots~\ref[pca-plot]).
Therefore, all fluxes of each spectrum are scaled.

Two popular forms of scaling are bound fluxes into range (min-max scaling)
and standardization of spectrum to have zero mean and unit variance.
As show in historgrams~\ref[fluxes-hist]
originaly the fluxes are in range $(378.23, -0.11)$,
min-max scaling would bound them to chosen range
and after standartization they are in range $(8.25, -8.15)$.

\midinsert \clabel[fluxes-hist]{Histogram of fluxes}
\picw=15cm \cinspic img/fluxes-hist.png
\caption/f Histograms of maxima and minima spectra fluxes before and after
standardization to zero mean and unit variance.
\endinsert

The two methods seems to work similar.
In this work scaling to zero mean and unit variance is used.
Scaling of spectra fluxes is done with Scikit-learn 0.18.1 \cite[sklearn]:

\begtt
import sklearn.preprocessing

X_scaled = sklearn.preprocessing.scale(X_original, axis=1)
\endtt

where {\tt X\_original} is input matrix of size $n \times 140$
($n$ is number of spectra samples) containing original dataset.
Each row of the matrix are the 140 fluxes of a spectrum.
